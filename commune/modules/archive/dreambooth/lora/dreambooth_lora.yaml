pretrained_model_name_or_path: stabilityai/stable-diffusion-2-1  # Path to pretrained model or model identifier from huggingface.co/models. (required)
revision: null  # Revision of pretrained model identifier from huggingface.co/models.
tokenizer_name: null  # Pretrained tokenizer name or path if not the same as model_name
instance_data_dir: ./dog  # A folder containing the training data of instance images. (required)
class_data_dir: null  # A folder containing the training data of class images.
instance_prompt: 'a dog'  # The prompt with identifier specifying the instance. (required)
validation_prompt: 'a dog'  # The prompt with identifier specifying the instance for validation.
class_prompt: null  # The prompt to specify images in the same class as provided instance images.
num_validation_images: 4  # Number of images that should be generated during validation with `validation_prompt`.
validation_epochs: 50  # Run dreambooth validation every X epochs.
with_prior_preservation: true  # Flag to add prior preservation loss.
prior_loss_weight: 1.0  # The weight of prior preservation loss.
num_class_images: 100  # Minimal class images for prior preservation loss.
output_dir: lora-dreambooth-model  # The output directory where the model predictions and checkpoints will be written.
logging_dir: logs
mixed_precision: fp16
report_to: tensorboard
allow_tf32: false  # Whether or not to allow TF32 on NVIDIA Ampere GPUs.
enable_xformers_memory_efficient_attention: false
seed: 1337   # A seed for reproducible training.
resolution: 512  # The resolution for input images.
center_crop: false  # Whether to center crop the input images to the resolution.
train_batch_size: 4  # Batch size (per device) for the training dataloader.
sample_batch_size: 4  # Batch size (per device) for sampling images.
num_train_epochs: 1 # Total number of training epochs to perform
max_train_steps: null  # Total number of training steps to perform.
checkpointing_steps: 500  # Save a checkpoint of the training state every X updates.
checkpoints_total_limit: null  # Max number of checkpoints to store.
resume_from_checkpoint: null  # Whether training should be resumed from a previous checkpoint.
gradient_accumulation_steps: 1  # Number of updates steps to accumulate before performing a backward/update pass.
gradient_checkpointing: false  # Whether or not to use gradient checkpointing to save memory.
learning_rate: 0.0005  # Initial learning rate to use.
scale_lr: false  # Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.
lr_scheduler: cosine  # The scheduler type to use.
lr_warmup_steps: 500  # Number of steps for the warmup in the lr scheduler.
lr_num_cycles: 1  # Number of hard resets of the lr in cosine_with_restarts scheduler.
lr_power: 1.0  # Power factor of the polynomial scheduler.
dataloader_num_workers: 8  # Number of subprocesses to use for data loading.
use_8bit_adam: false  # Whether or not to use 8-bit Adam from bitsandbytes.
adam_beta1: 0.9  # The beta1 parameter for the Adam optimizer.
adam_beta2: 0.999  # The beta2 parameter for the Adam optimizer.
adam_weight_decay: 0.01  # Weight decay to use.
adam_epsilon: 0.00000001  # Epsilon value for the Adam optimizer.
max_grad_norm: 1.0  # Max gradient norm.
push_to_hub: false  # Whether or not to push the model to the Hub.
hub_token: null  # The token to use to push to the Model Hub.
