pretrained_model_name_or_path: null  # Path to pretrained model or model identifier from huggingface.co/models.
revision: null  # Revision of pretrained model identifier from huggingface.co/models. Trainable model components should be float32 precision.
tokenizer_name: null  # Pretrained tokenizer name or path if not the same as model_name
instance_data_dir: null  # A folder containing the training data of instance images.
class_data_dir: null  # A folder containing the training data of class images.
instance_prompt: null  # The prompt with identifier specifying the instance
class_prompt: null  # The prompt to specify images in the same class as provided instance images.
with_prior_preservation: false  # Flag to add prior preservation loss.
prior_loss_weight: 1.0  # The weight of prior preservation loss.
num_class_images: 100  # Minimal class images for prior preservation loss. If there are not enough images already present in class_data_dir, additional images will be sampled with class_prompt.
output_dir: text-inversion-model  # The output directory where the model predictions and checkpoints will be written.
seed: null  # A seed for reproducible training.
resolution: 512  # The resolution for input images, all the images in the train/validation dataset will be resized to this resolution
center_crop: false  # Whether to center crop the input images to the resolution. If not set, the images will be randomly cropped. The images will be resized to the resolution first before cropping.
train_text_encoder: false  # Whether to train the text encoder. If set, the text encoder should be float32 precision.
train_batch_size: 4  # Batch size (per device) for the training dataloader.
sample_batch_size: 4  # Batch size (per device) for sampling images.
num_train_epochs: 1
max_train_steps: null  # Total number of training steps to perform. If provided, overrides num_train_epochs.
checkpointing_steps: 500  # Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training via `--resume_from_checkpoint`. In the case that the checkpoint is better than the final trained model, the checkpoint can also be used for inference. Using a checkpoint for inference requires separate loading of the original pipeline and the individual checkpointed model components. See https://huggingface.co/docs/diffusers/main/en/training/dreambooth#performing-inference-using-a-saved-checkpoint for step by step instructions.
checkpoints_total_limit: null  # Max number of checkpoints to store. Passed as `total_limit` to the `Accelerator` `ProjectConfiguration`. See Accelerator::save_state https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.save_state for more details
resume_from_checkpoint: null  # Whether training should be resumed from a previous checkpoint. Use a path saved by `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.
gradient_accumulation_steps: 1  # Number of updates steps to accumulate before performing a backward/update pass.
gradient_checkpointing: false  # Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.
learning_rate: 5e-6  # Initial learning rate (after the potential warmup period) to use.
scale_lr: false  # Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.
lr_scheduler: constant  # The scheduler type to use. Choose between ["linear", "cosine",
