vocab_size: 32001
hidden_size: 4096
intermediate_size: 11008
num_hidden_layers: 2
tokenizer: 'lmsys/vicuna-7b-delta-v0'
num_attention_heads: 32
hidden_act: "silu"
max_position_embeddings: 2048
initializer_range: 0.02
rms_norm_eps: 0.000001
use_cache: true
pad_token_id: 0
bos_token_id: 1
eos_token_id: 2
tie_word_embeddings: false
output_attentions: false
output_hidden_states: false
device: [0,1]
load_weights: decapoda-research/llama-7b-hf
