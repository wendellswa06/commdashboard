{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Transformer Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import commune as c\n",
    "c.enable_jupyter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the model.transformers Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__init__': {'input': {'config': 'NA'}, 'output': {}, 'type': 'self'},\n",
       " 'forward': {'input': {'input_ids': 'union[str, torch',\n",
       "   'attention_mask': 'torch.Tensor',\n",
       "   'return_keys': 'list[str]',\n",
       "   'topk': 'int',\n",
       "   'hidden_layer': 'int',\n",
       "   'max_sequence_length': 'int'},\n",
       "  'output': {},\n",
       "  'type': 'self'},\n",
       " 'encode': {'input': {'text': 'str', 'token_idx': 'int'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'self'},\n",
       " 'embed': {'input': {'text': 'str', 'token_idx': 'int'},\n",
       "  'output': {},\n",
       "  'type': 'self'},\n",
       " 'set_model': {'input': {'config': 'NA'}, 'output': 'None', 'type': 'self'},\n",
       " 'set_tokenizer': {'input': {'tokenizer': 'str'},\n",
       "  'output': {},\n",
       "  'type': 'self'},\n",
       " 'encode_topk': {'input': {'forward_response_tensor': 'torch.Tensor',\n",
       "   'topk': 'int'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'static'},\n",
       " 'decode_topk': {'input': {'forward_response_tensor': 'torch.Tensor',\n",
       "   'vocab_size': 'int',\n",
       "   'topk': 'NA'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'static'},\n",
       " 'tokenizer_name': {'input': {}, 'output': {}, 'type': 'self'},\n",
       " 'tokenize': {'input': {'text': 'str',\n",
       "   'device': 'str',\n",
       "   'padding': 'NA',\n",
       "   'truncation': 'NA',\n",
       "   'max_length': 'NA',\n",
       "   'return_tensors': 'NA',\n",
       "   'add_special_tokens': 'NA'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'self'},\n",
       " 'detokenize': {'input': {'input_ids': 'torch.Tensor'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'self'},\n",
       " 'sample_check': {'input': {'sample': 'NA'}, 'output': {}, 'type': 'cls'},\n",
       " 'async_get_sample': {'input': {'dataset': 'NA',\n",
       "   'max_trials': 'NA',\n",
       "   'batch_size': 'NA',\n",
       "   'sequence_length': 'NA',\n",
       "   'num_batches': 'NA'},\n",
       "  'output': {},\n",
       "  'type': 'cls'},\n",
       " 'get_sample': {'input': {'timeout': 'NA', 'retries': 'NA'},\n",
       "  'output': {},\n",
       "  'type': 'cls'},\n",
       " 'resolve_model': {'input': {'model': 'NA'}, 'output': {}, 'type': 'cls'},\n",
       " 'test_encode': {'input': {'num_samples': 'int', 'text': 'NA'},\n",
       "  'output': {},\n",
       "  'type': 'cls'},\n",
       " 'serve': {'input': {'model': 'str', 'tag': 'NA', 'refresh': 'NA'},\n",
       "  'output': {},\n",
       "  'type': 'cls'},\n",
       " 'calculate_loss': {'input': {'logits': 'torch.Tensor',\n",
       "   'input_ids': 'torch.Tensor',\n",
       "   'return_value': 'NA'},\n",
       "  'output': 'torch.Tensor',\n",
       "  'type': 'cls'},\n",
       " 'generate': {'input': {'text': 'str',\n",
       "   'max_length': 'int',\n",
       "   'max_new_tokens': 'int',\n",
       "   'min_length': 'int',\n",
       "   'min_new_tokens': 'int',\n",
       "   'early_stopping': 'bool',\n",
       "   'max_time': 'float'},\n",
       "  'output': 'list[str]',\n",
       "  'type': 'self'},\n",
       " 'test_generate': {'input': {}, 'output': {}, 'type': 'cls'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = c.module('model.hf')\n",
    "# see the schema if you want\n",
    "model.schema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Model shortcuts\n",
    "\n",
    "Managing module shortcuts help you simplify the naming of long ass hf model paths. We have done it for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oa.pythia.12b': 'OpenAssistant/oasst-sft-1-pythia-12b',\n",
       " 'vicuna.33b': 'lmsys/vicuna-33b-v1.3',\n",
       " 'vicuna.13b': 'lmsys/vicuna-13b-v1.3',\n",
       " 'vicuna.7b': 'lmsys/vicuna-7b-v1.3',\n",
       " 'llama': 'openlm-research/open_llama_7b',\n",
       " 'llama2.70b': 'meta-llama/Llama-2-70b-chat-hf',\n",
       " 'llama2.7b': 'meta-llama/Llama-2-7b',\n",
       " 'llama13b': 'openlm-research/open_llama_13b',\n",
       " 'llama-trl': 'trl-lib/llama-7b-se-rl-peft',\n",
       " 'pygmalion-6b': 'Pygmalionreplit/replit-code-v1-3bAI/pygmalion-6b',\n",
       " 'stablellm7b': 'StabilityAI/stablelm-tuned-alpha-7b',\n",
       " 'mosaic': 'mosaicml/mpt-30b',\n",
       " 'falcon_40b_instruct': 'tiiuae/falcon-40b-instruct',\n",
       " 'falcon_uncensored': 'ehartford/WizardLM-Uncensored-Falcon-40b',\n",
       " 'falcon': 'tiiuae/falcon-40b-instruct',\n",
       " 'open_llama_13b': 'openlm-research/open_llama_13b',\n",
       " 'lazarus30b': 'CalderaAI/30B-Lazarus',\n",
       " 'starcoder': 'bigcode/starcoder',\n",
       " 'wizardcoder': 'WizardLM/WizardCoder-15B-V1.0',\n",
       " 'replit_code': 'replit/replit-code-v1-3b'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.shortcuts.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shortcut = 'gpt2'\n",
    "path = 'OpenAI/gpt-2'\n",
    "c.setc(f'shortcuts.{shortcut}', path) \n",
    "assert c.getc(f'shortcuts.{shortcut}') == path\n",
    "c.rmc(f'shortcuts.{shortcut}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Serving model.transformer as model.vicuna.</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">7b::c</span><span style=\"color: #808000; text-decoration-color: #808000\">ool</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mServing model.transformer as model.vicuna.\u001b[0m\u001b[1;92m7b::c\u001b[0m\u001b[33mool\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'module'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'model.hf'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'tag'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'server_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'model.vicuna.7b::cool'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'config'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Munch</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'vicuna.7b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tag'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'cool'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'device'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'device_map'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'auto'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'max_memory'</span>: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'trust_remote_code'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'finetune'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'optimizer'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Munch</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2e-05</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'module'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'torch.optim.Adam'</span><span style=\"font-weight: bold\">})</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'load_in_8bit'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'max_sequence_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'load'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'block_prefix'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'model.layers'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'model2block_prefix'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Munch</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008000; text-decoration-color: #008000\">'mosaic'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'transformer.blocks'</span><span style=\"font-weight: bold\">})</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'wait_for_server'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">})</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'verbose'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'refresh'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'wait_for_server'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'remote'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'server_mode'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'http'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'tag_seperator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'::'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'extra_kwargs'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'module'\u001b[0m: \u001b[32m'model.hf'\u001b[0m,\n",
       "    \u001b[32m'tag'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'server_name'\u001b[0m: \u001b[32m'model.vicuna.7b::cool'\u001b[0m,\n",
       "    \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'config'\u001b[0m: \u001b[1;35mMunch\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'model'\u001b[0m: \u001b[32m'vicuna.7b'\u001b[0m, \u001b[32m'tag'\u001b[0m: \u001b[32m'cool'\u001b[0m, \u001b[32m'device'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'device_map'\u001b[0m: \u001b[32m'auto'\u001b[0m, \u001b[32m'max_memory'\u001b[0m: \n",
       "\u001b[3;35mNone\u001b[0m, \u001b[32m'trust_remote_code'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'finetune'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'optimizer'\u001b[0m: \u001b[1;35mMunch\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'lr'\u001b[0m: \u001b[1;36m2e-05\u001b[0m, \u001b[32m'module'\u001b[0m: \u001b[32m'torch.optim.Adam'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m, \n",
       "\u001b[32m'load_in_8bit'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'max_sequence_length'\u001b[0m: \u001b[1;36m256\u001b[0m, \u001b[32m'load'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'block_prefix'\u001b[0m: \u001b[32m'model.layers'\u001b[0m, \n",
       "\u001b[32m'model2block_prefix'\u001b[0m: \u001b[1;35mMunch\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'mosaic'\u001b[0m: \u001b[32m'transformer.blocks'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'wait_for_server'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'verbose'\u001b[0m: \u001b[3;92mTrue\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'refresh'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[32m'wait_for_server'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'remote'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[32m'server_mode'\u001b[0m: \u001b[32m'http'\u001b[0m,\n",
       "    \u001b[32m'tag_seperator'\u001b[0m: \u001b[32m'::'\u001b[0m,\n",
       "    \u001b[32m'extra_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">class:c</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">name</span><span style=\"color: #008000; text-decoration-color: #008000\">:model.vicuna.</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">7b::c</span><span style=\"color: #008000; text-decoration-color: #008000\">ool </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">fn</span><span style=\"color: #008000; text-decoration-color: #008000\">:serve </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">mode</span><span style=\"color: #008000; text-decoration-color: #008000\">:pm2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mLaunching\u001b[0m\u001b[32m \u001b[0m\u001b[1;33mclass:c\u001b[0m\u001b[32m \u001b[0m\u001b[1;37mname\u001b[0m\u001b[32m:model.vicuna.\u001b[0m\u001b[1;92m7b::c\u001b[0m\u001b[32mool \u001b[0m\u001b[1;37mfn\u001b[0m\u001b[32m:serve \u001b[0m\u001b[1;37mmode\u001b[0m\u001b[32m:pm2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'model.vicuna.7b::cool'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.serve('vicuna.7b', tag='cool', wait_for_server=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!c restart model.vicuna.7b::cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/home/endless/.local/lib/python3.10/site-packages/aiofiles/threadpool/__init__.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97</span>, in _open\n",
       "    f = yield from <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">loop.run_in_executor</span><span style=\"font-weight: bold\">(</span>executor, cb<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/usr/lib/python3.10/asyncio/base_events.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">821</span>, in run_in_executor\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">executor.submit</span><span style=\"font-weight: bold\">(</span>func, *args<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">loop</span>=<span style=\"color: #800080; text-decoration-color: #800080\">self</span><span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/usr/lib/python3.10/concurrent/futures/thread.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">167</span>, in submit\n",
       "    raise <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RuntimeError</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'cannot schedule new futures after shutdown'</span><span style=\"font-weight: bold\">)</span>\n",
       "RuntimeError: cannot schedule new futures after shutdown\n",
       "You are using the legacy behaviour of the <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.llama.tokenization_llama.LlamaTokenizer'</span><span style=\"font-weight: bold\">&gt;</span>. \n",
       "This means that tokens that come after special tokens will not be properly handled. We recommend you to read the \n",
       "related pull request available at <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers/pull/24565</span>\n",
       "You are using the legacy behaviour of the <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.llama.tokenization_llama.LlamaTokenizer'</span><span style=\"font-weight: bold\">&gt;</span>. \n",
       "This means that tokens that come after special tokens will not be properly handled. We recommend you to read the \n",
       "related pull request available at <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/huggingface/transformers/pull/24565</span>\n",
       "Loading checkpoint shards:   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>%|          | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">00:00</span>&lt;?, ?it/s<span style=\"font-weight: bold\">]</span>Loading checkpoint shards:   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>%|          | <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">00:00</span>&lt;?, ?it/s<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  File \u001b[32m\"/home/endless/.local/lib/python3.10/site-packages/aiofiles/threadpool/__init__.py\"\u001b[0m, line \u001b[1;36m97\u001b[0m, in _open\n",
       "    f = yield from \u001b[1;35mloop.run_in_executor\u001b[0m\u001b[1m(\u001b[0mexecutor, cb\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/usr/lib/python3.10/asyncio/base_events.py\"\u001b[0m, line \u001b[1;36m821\u001b[0m, in run_in_executor\n",
       "    \u001b[1;35mexecutor.submit\u001b[0m\u001b[1m(\u001b[0mfunc, *args\u001b[1m)\u001b[0m, \u001b[33mloop\u001b[0m=\u001b[35mself\u001b[0m\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/usr/lib/python3.10/concurrent/futures/thread.py\"\u001b[0m, line \u001b[1;36m167\u001b[0m, in submit\n",
       "    raise \u001b[1;35mRuntimeError\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'cannot schedule new futures after shutdown'\u001b[0m\u001b[1m)\u001b[0m\n",
       "RuntimeError: cannot schedule new futures after shutdown\n",
       "You are using the legacy behaviour of the \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'transformers.models.llama.tokenization_llama.LlamaTokenizer'\u001b[0m\u001b[1m>\u001b[0m. \n",
       "This means that tokens that come after special tokens will not be properly handled. We recommend you to read the \n",
       "related pull request available at \u001b[4;94mhttps://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
       "You are using the legacy behaviour of the \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'transformers.models.llama.tokenization_llama.LlamaTokenizer'\u001b[0m\u001b[1m>\u001b[0m. \n",
       "This means that tokens that come after special tokens will not be properly handled. We recommend you to read the \n",
       "related pull request available at \u001b[4;94mhttps://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
       "Loading checkpoint shards:   \u001b[1;36m0\u001b[0m%|          | \u001b[1;36m0\u001b[0m/\u001b[1;36m2\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<?, ?it/s\u001b[1m]\u001b[0mLoading checkpoint shards:   \u001b[1;36m0\u001b[0m%|          | \u001b[1;36m0\u001b[0m/\u001b[1;36m2\u001b[0m \n",
       "\u001b[1m[\u001b[0m\u001b[1;92m00:00\u001b[0m<?, ?it/s\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c.print(c.logs(\"model.vicuna.7b::cool\", mode='local'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
